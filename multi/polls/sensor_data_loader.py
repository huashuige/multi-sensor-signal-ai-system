import numpy as np
import pandas as pd
from django.db import connection
from .models import MonitorTask
from .multi_channel_lstm import MultiChannelSensorPreprocessor, MultiChannelLSTM, MultiChannelTrainer, create_data_loaders
import torch
import os
import json
from datetime import datetime

class SensorDataLoader:
    """ä¼ æ„Ÿå™¨æ•°æ®åŠ è½½å™¨"""
    
    def __init__(self):
        # æ”¯æŒCSVæ–‡ä»¶ä¸­çš„é€šé“åˆ—åï¼ˆCH0, CH1, CH2, ...ï¼‰
        self.channel_mapping = {}
        for i in range(16):  # æ”¯æŒ16ä¸ªé€šé“
            self.channel_mapping[f'CH{i}'] = f'CH{i}'
        
        # ä¹Ÿæ”¯æŒä¼ ç»Ÿçš„ä¼ æ„Ÿå™¨åç§°ï¼ˆå‘åå…¼å®¹ï¼‰
        traditional_channels = {
            'temperature': 'temperature',
            'humidity': 'humidity', 
            'acceleration': 'acceleration',
            'voltage': 'voltage',
            'current': 'current',
            'power': 'power',
            'frequency': 'frequency',
            'pressure': 'pressure',
            'flow_rate': 'flow_rate',
            'level': 'level'
        }
        self.channel_mapping.update(traditional_channels)
    
    def load_sensor_data(self, task_ids, channels, start_time=None, end_time=None):
        """
        ä»CSVæ–‡ä»¶åŠ è½½å¤šé€šé“ä¼ æ„Ÿå™¨æ•°æ®
        
        Args:
            task_ids: ä»»åŠ¡IDåˆ—è¡¨
            channels: é€šé“åç§°åˆ—è¡¨ï¼Œå¦‚ ['temperature', 'humidity', 'acceleration']
            start_time: å¼€å§‹æ—¶é—´ (datetimeå¯¹è±¡)
            end_time: ç»“æŸæ—¶é—´ (datetimeå¯¹è±¡)
        
        Returns:
            data: numpyæ•°ç»„ï¼Œå½¢çŠ¶ä¸º (channels, timesteps)
            channel_names: é€šé“åç§°åˆ—è¡¨
        """
        print(f"å¼€å§‹åŠ è½½ä¼ æ„Ÿå™¨æ•°æ®...")
        print(f"ä»»åŠ¡ID: {task_ids}")
        print(f"é€šé“: {channels}")
        
        # éªŒè¯é€šé“
        valid_channels = []
        for channel in channels:
            if channel in self.channel_mapping:
                valid_channels.append(channel)
            else:
                print(f"è­¦å‘Š: æœªçŸ¥é€šé“ '{channel}'ï¼Œå·²è·³è¿‡")
        
        if not valid_channels:
            raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„é€šé“")
        
        # ä»ä»»åŠ¡ä¸­è·å–CSVæ–‡ä»¶è·¯å¾„
        all_data = []
        channel_names = []
        
        for task_id in task_ids:
            try:
                task = MonitorTask.objects.get(task_id=task_id)
                csv_file_path = task.csv_file_path
                
                if not os.path.exists(csv_file_path):
                    print(f"è­¦å‘Š: CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_file_path}")
                    continue
                
                # è¯»å–CSVæ–‡ä»¶
                df = pd.read_csv(csv_file_path)
                print(f"ä» {csv_file_path} åŠ è½½æ•°æ®ï¼Œå½¢çŠ¶: {df.shape}")
                
                # æå–æŒ‡å®šé€šé“çš„æ•°æ®
                for channel in valid_channels:
                    if channel in df.columns:
                        channel_data = df[channel].values
                        all_data.append(channel_data)
                        channel_names.append(f"{task_id}_{channel}")
                    else:
                        print(f"è­¦å‘Š: CSVæ–‡ä»¶ä¸­æ²¡æœ‰ '{channel}' åˆ—")
                
            except MonitorTask.DoesNotExist:
                print(f"è­¦å‘Š: ä»»åŠ¡ID '{task_id}' ä¸å­˜åœ¨")
            except Exception as e:
                print(f"è­¦å‘Š: åŠ è½½ä»»åŠ¡ '{task_id}' æ•°æ®æ—¶å‡ºé”™: {e}")
        
        if not all_data:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ•°æ®")
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        data = np.array(all_data)
        
        print(f"æ•°æ®åŠ è½½å®Œæˆ: {data.shape[0]}ä¸ªé€šé“, {data.shape[1]}ä¸ªæ—¶é—´ç‚¹")
        print(f"é€šé“åç§°: {channel_names}")
        
        return data, channel_names
    
    def load_data_by_task(self, task_id, channels=None):
        """
        æ ¹æ®ä»»åŠ¡IDåŠ è½½æ•°æ®
        
        Args:
            task_id: ä»»åŠ¡ID
            channels: é€šé“åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™åŠ è½½æ‰€æœ‰å¯ç”¨é€šé“
        
        Returns:
            data: numpyæ•°ç»„
            channel_names: é€šé“åç§°åˆ—è¡¨
        """
        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å­˜åœ¨
        try:
            task = MonitorTask.objects.get(task_id=task_id)
            print(f"ä»»åŠ¡ä¿¡æ¯: {task.task_name}")
        except MonitorTask.DoesNotExist:
            raise ValueError(f"ä»»åŠ¡ID '{task_id}' ä¸å­˜åœ¨")
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šé€šé“ï¼Œåˆ™è·å–æ‰€æœ‰å¯ç”¨é€šé“
        if channels is None:
            channels = self._get_available_channels(task_id)
        
        return self.load_sensor_data([task_id], channels)
    
    def _get_available_channels(self, task_id):
        """è·å–ä»»åŠ¡ä¸­å¯ç”¨çš„é€šé“"""
        try:
            task = MonitorTask.objects.get(task_id=task_id)
            csv_file_path = task.csv_file_path
            
            if not os.path.exists(csv_file_path):
                print(f"è­¦å‘Š: CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_file_path}")
                return []
            
            # è¯»å–CSVæ–‡ä»¶å¤´éƒ¨æ¥è·å–å¯ç”¨åˆ—
            df = pd.read_csv(csv_file_path, nrows=1)  # åªè¯»å–ç¬¬ä¸€è¡Œæ¥è·å–åˆ—å
            
            available_channels = []
            # æŸ¥æ‰¾æ‰€æœ‰CHå¼€å¤´çš„åˆ—ï¼ˆå¦‚CH0, CH1, CH2, ...ï¼‰
            for column in df.columns:
                if column.startswith('CH') and column[2:].isdigit():
                    available_channels.append(column)
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°CHæ ¼å¼çš„åˆ—ï¼Œå°è¯•æŸ¥æ‰¾ä¼ ç»Ÿçš„ä¼ æ„Ÿå™¨åç§°
            if not available_channels:
                for channel in self.channel_mapping.keys():
                    if channel in df.columns:
                        available_channels.append(channel)
            
            print(f"å¯ç”¨é€šé“: {available_channels}")
            return available_channels
            
        except MonitorTask.DoesNotExist:
            print(f"è­¦å‘Š: ä»»åŠ¡ID '{task_id}' ä¸å­˜åœ¨")
            return []
        except Exception as e:
            print(f"è­¦å‘Š: è·å–å¯ç”¨é€šé“æ—¶å‡ºé”™: {e}")
            return []

def train_multi_channel_model(task_ids, channels, model_config=None, save_dir="models", progress_callback=None, training_set_id=None):
    """
    è®­ç»ƒå¤šé€šé“LSTMæ¨¡å‹
    
    Args:
        task_ids: ä»»åŠ¡IDåˆ—è¡¨
        channels: é€šé“åç§°åˆ—è¡¨
        model_config: æ¨¡å‹é…ç½®å­—å…¸
        save_dir: æ¨¡å‹ä¿å­˜ç›®å½•
        progress_callback: è®­ç»ƒè¿›åº¦å›è°ƒå‡½æ•°
        training_set_id: è®­ç»ƒé›†IDï¼Œç”¨äºæ£€æŸ¥åœæ­¢çŠ¶æ€
    
    Returns:
        trainer: è®­ç»ƒå¥½çš„è®­ç»ƒå™¨å¯¹è±¡
        metadata: æ¨¡å‹å…ƒæ•°æ®
    """
    print(f"ğŸš€ å¼€å§‹å¤šé€šé“LSTMæ¨¡å‹è®­ç»ƒ...")
    print(f"ğŸ“Š ä»»åŠ¡ID: {task_ids}")
    print(f"ğŸ“Š é€šé“: {channels}")
    print(f"ğŸ“Š æ¨¡å‹é…ç½®: {model_config}")
    print(f"ğŸ“ ä¿å­˜ç›®å½•: {save_dir}")
    if training_set_id:
        print(f"ğŸ†” è®­ç»ƒé›†ID: {training_set_id}")
    
    try:
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        print(f"ğŸ“¦ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
        loader = SensorDataLoader()
        
        # åŠ è½½æ•°æ®
        print(f"ğŸ“¥ å¼€å§‹åŠ è½½ä¼ æ„Ÿå™¨æ•°æ®...")
        data, channel_names = loader.load_sensor_data(task_ids, channels)
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼Œå½¢çŠ¶: {data.shape}")
        print(f"âœ… é€šé“åç§°: {channel_names}")
        
        # æ£€æŸ¥æ•°æ®è´¨é‡
        print(f"ğŸ” æ£€æŸ¥æ•°æ®è´¨é‡...")
        invalid_channels = []
        for i, channel in enumerate(channel_names):
            channel_data = data[i, :]
            min_val = channel_data.min()
            max_val = channel_data.max()
            mean_val = channel_data.mean()
            std_val = channel_data.std()
            
            print(f"  {channel}: å½¢çŠ¶={channel_data.shape}, èŒƒå›´=[{min_val:.3f}, {max_val:.3f}], å‡å€¼={mean_val:.3f}, æ ‡å‡†å·®={std_val:.3f}")
            
            # æ£€æŸ¥æ•°æ®æ˜¯å¦æœ‰æ•ˆ
            if max_val == min_val:
                print(f"    âš ï¸  è­¦å‘Š: é€šé“æ•°æ®æ— æ•ˆï¼ˆæ‰€æœ‰å€¼ç›¸åŒï¼‰")
                invalid_channels.append(i)
        
        # å¦‚æœæ‰€æœ‰é€šé“éƒ½æ— æ•ˆï¼Œæå‰åœæ­¢
        if len(invalid_channels) == len(channel_names):
            raise ValueError("æ‰€æœ‰é€šé“æ•°æ®éƒ½æ— æ•ˆï¼ˆæ‰€æœ‰å€¼ç›¸åŒï¼‰ï¼Œæ— æ³•è¿›è¡Œè®­ç»ƒã€‚è¯·æ£€æŸ¥ä¼ æ„Ÿå™¨è¿æ¥æˆ–ä½¿ç”¨æœ‰æ•ˆçš„æ•°æ®ã€‚")
        
        # åˆ›å»ºé¢„å¤„ç†å™¨
        print(f"ğŸ”§ åˆ›å»ºæ•°æ®é¢„å¤„ç†å™¨...")
        window_size = model_config.get('window_size', 24)
        horizon = model_config.get('horizon', 12)
        preprocessor = MultiChannelSensorPreprocessor(window_size=window_size, horizon=horizon)
        
        # é¢„å¤„ç†æ•°æ®
        print(f"ğŸ”„ å¼€å§‹æ•°æ®é¢„å¤„ç†...")
        processed_data = preprocessor.fit_transform(data)
        print(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")
        print(f"ğŸ“Š è®­ç»ƒæ•°æ®å½¢çŠ¶: {processed_data['X_train'].shape}")
        print(f"ğŸ“Š éªŒè¯æ•°æ®å½¢çŠ¶: {processed_data['X_val'].shape}")
        print(f"ğŸ“Š æµ‹è¯•æ•°æ®å½¢çŠ¶: {processed_data['X_test'].shape}")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        print(f"ğŸ“¦ åˆ›å»ºPyTorchæ•°æ®åŠ è½½å™¨...")
        batch_size = model_config.get('batch_size', 32)
        
        print(f"ğŸ“Š æ‰¹æ¬¡å¤§å°: {batch_size}")
        
        train_loader, val_loader = create_data_loaders(
            processed_data['X_train'], 
            processed_data['y_train'],
            processed_data['X_val'], 
            processed_data['y_val'],
            batch_size=batch_size
        )
        
        print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ")
        print(f"  - è®­ç»ƒé›†æ‰¹æ¬¡æ•°: {len(train_loader)}")
        print(f"  - éªŒè¯é›†æ‰¹æ¬¡æ•°: {len(val_loader)}")
        
        # åˆ›å»ºæ¨¡å‹
        print(f"ğŸ—ï¸ åˆ›å»ºLSTMæ¨¡å‹...")
        input_size = processed_data['num_channels']  # é€šé“æ•°
        hidden_size = model_config.get('hidden_size', 64)
        num_layers = model_config.get('num_layers', 2)
        dropout = model_config.get('dropout', 0.1)
        horizon = model_config.get('horizon', 12)
        
        print(f"ğŸ“Š è¾“å…¥ç»´åº¦: {input_size}")
        print(f"ğŸ“Š éšè—å±‚å¤§å°: {hidden_size}")
        print(f"ğŸ“Š LSTMå±‚æ•°: {num_layers}")
        print(f"ğŸ“Š Dropoutç‡: {dropout}")
        print(f"ğŸ“Š é¢„æµ‹æ­¥é•¿: {horizon}")
        
        # åˆ›å»ºLSTMæ¨¡å‹
        model = MultiChannelLSTM(
            num_channels=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            horizon=horizon,
            dropout=dropout
        )
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = MultiChannelTrainer(model)
        
        # å¼€å§‹è®­ç»ƒ
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒ...")
        history = trainer.train(
            train_loader, 
            val_loader, 
            epochs=model_config.get('epochs', 100),
            lr=model_config.get('lr', 0.001),
            patience=model_config.get('patience', 10),
            progress_callback=progress_callback,
            training_set_id=training_set_id
        )
        
        # æ£€æŸ¥è®­ç»ƒæ˜¯å¦è¢«åœæ­¢
        if history.get('status') == 'stopped':
            print(f"â¹ï¸ è®­ç»ƒå·²è¢«åœæ­¢")
            return trainer, {
                'model_path': None,
                'channels': channel_names,
                'data_info': {
                    'window_size': window_size,
                    'horizon': horizon,
                    'input_size': input_size,
                    'data_shape': data.shape
                },
                'model_config': model_config,
                'training_history': history,
                'channel_stats': processed_data['channel_stats'],
                'test_data': {
                    'X_test': processed_data['X_test'],
                    'y_test': processed_data['y_test']
                },
                'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S'),
                'status': 'stopped'
            }
        
        print(f"âœ… è®­ç»ƒå®Œæˆï¼")
        print(f"â° è®­ç»ƒç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # ä¿å­˜æ¨¡å‹
        print(f"ğŸ’¾ ä¿å­˜æ¨¡å‹...")
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        model_filename = f"multi_channel_lstm_{timestamp}.pth"
        model_path = os.path.join(save_dir, model_filename)
        
        metadata = {
            'model_path': model_path,
            'channels': channel_names,
            'data_info': {
                'window_size': window_size,
                'horizon': horizon,
                'input_size': input_size,
                'data_shape': data.shape
            },
            'model_config': model_config,
            'training_history': history,
            'channel_stats': processed_data['channel_stats'],
            'test_data': {
                'X_test': processed_data['X_test'].tolist(),  # è½¬æ¢ä¸ºåˆ—è¡¨
                'y_test': processed_data['y_test'].tolist()   # è½¬æ¢ä¸ºåˆ—è¡¨
            },
            'timestamp': timestamp,
        }
        
        trainer.save_model(model_path, metadata)
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
        
        # æ‰“å°è®­ç»ƒç»“æœ
        print(f"ğŸ“Š è®­ç»ƒç»“æœæ€»ç»“:")
        if history and 'train_loss' in history and len(history['train_loss']) > 0:
            print(f"  - æœ€ç»ˆè®­ç»ƒæŸå¤±: {history['train_loss'][-1]:.6f}")
        if history and 'val_loss' in history and len(history['val_loss']) > 0:
            print(f"  - æœ€ç»ˆéªŒè¯æŸå¤±: {history['val_loss'][-1]:.6f}")
        if history and 'best_val_loss' in history:
            print(f"  - æœ€ä½³éªŒè¯æŸå¤±: {history['best_val_loss']:.6f}")
        if history and 'epochs_trained' in history:
            print(f"  - è®­ç»ƒè½®æ•°: {history['epochs_trained']}")
        
        return trainer, metadata
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        raise e

def load_trained_model(model_path):
    """
    åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    
    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
    
    Returns:
        trainer: è®­ç»ƒå™¨å¯¹è±¡
        metadata: æ¨¡å‹å…ƒæ•°æ®
    """
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
    
    # é¦–å…ˆåŠ è½½æ¨¡å‹æ•°æ®æ¥è·å–é…ç½®
    save_data = torch.load(model_path, map_location='cpu', weights_only=False)
    model_config = save_data.get('model_config', {})
    
    # ä»æ¨¡å‹é…ç½®ä¸­è·å–é€šé“æ•°ï¼Œå¿…é¡»å­˜åœ¨
    if 'num_channels' not in model_config:
        raise ValueError(f"æ¨¡å‹é…ç½®ä¸­ç¼ºå°‘num_channelså­—æ®µ: {model_config}")
    
    num_channels = model_config['num_channels']
    hidden_size = model_config.get('hidden_size', 64)
    num_layers = model_config.get('num_layers', 2)
    horizon = model_config.get('horizon', 12)
    
    print(f"ğŸ“Š ä»æ¨¡å‹æ–‡ä»¶ä¸­è¯»å–çš„é…ç½®:")
    print(f"  - é€šé“æ•°: {num_channels}")
    print(f"  - éšè—å±‚å¤§å°: {hidden_size}")
    print(f"  - LSTMå±‚æ•°: {num_layers}")
    print(f"  - é¢„æµ‹æ­¥é•¿: {horizon}")
    
    # ä½¿ç”¨æ­£ç¡®çš„é…ç½®åˆ›å»ºdummyæ¨¡å‹
    dummy_model = MultiChannelLSTM(
        num_channels=num_channels,
        hidden_size=hidden_size,
        num_layers=num_layers,
        horizon=horizon
    )
    trainer = MultiChannelTrainer(dummy_model)
    
    # åŠ è½½æ¨¡å‹
    metadata = trainer.load_model(model_path)
    
    # ç¡®ä¿metadataåŒ…å«æ­£ç¡®çš„model_configï¼ˆä½¿ç”¨æ¨¡å‹æ–‡ä»¶ä¸­çš„é…ç½®ï¼‰
    metadata['model_config'] = model_config
    
    return trainer, metadata

def predict_with_model(model_path, input_sequence, denormalize=True):
    """
    ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
    
    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        input_sequence: è¾“å…¥åºåˆ—ï¼Œå½¢çŠ¶ä¸º (channels, window_size)
        denormalize: æ˜¯å¦åå½’ä¸€åŒ–é¢„æµ‹ç»“æœ
    
    Returns:
        prediction: é¢„æµ‹ç»“æœ
        metadata: æ¨¡å‹å…ƒæ•°æ®
    """
    # åŠ è½½æ¨¡å‹
    trainer, metadata = load_trained_model(model_path)
    model = trainer.model
    
    # ç¡®ä¿è¾“å…¥æ ¼å¼æ­£ç¡®
    if isinstance(input_sequence, np.ndarray):
        input_sequence = torch.FloatTensor(input_sequence)
    
    # æ·»åŠ batchç»´åº¦
    if input_sequence.dim() == 2:  # (channels, window_size)
        input_sequence = input_sequence.unsqueeze(0)  # (1, channels, window_size)
    
    # é¢„æµ‹
    model.eval()
    with torch.no_grad():
        prediction = model(input_sequence)
        prediction = prediction.squeeze(0).cpu().numpy()  # (channels, horizon)
    
    # åå½’ä¸€åŒ–
    if denormalize and 'channel_stats' in metadata:
        prediction = denormalize_prediction(prediction, metadata['channel_stats'])
    
    return prediction, metadata

def denormalize_prediction(prediction, channel_stats):
    """åå½’ä¸€åŒ–é¢„æµ‹ç»“æœ"""
    denormalized = prediction.copy()
    
    for channel in range(prediction.shape[0]):
        if str(channel) in channel_stats:
            stats = channel_stats[str(channel)]
            min_val = stats['min']
            max_val = stats['max']
            
            # åå½’ä¸€åŒ–: normalized * (max - min) + min
            denormalized[channel, :] = prediction[channel, :] * (max_val - min_val) + min_val
    
    return denormalized

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # ç¤ºä¾‹ï¼šè®­ç»ƒæ¨¡å‹
    task_ids = ['task1', 'task2']  # æ›¿æ¢ä¸ºå®é™…çš„ä»»åŠ¡ID
    channels = ['temperature', 'humidity', 'acceleration']  # æ›¿æ¢ä¸ºå®é™…çš„é€šé“
    
    try:
        trainer, metadata = train_multi_channel_model(task_ids, channels)
        print("è®­ç»ƒæˆåŠŸå®Œæˆï¼")
        
        # ç¤ºä¾‹ï¼šä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹
        # å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªè¾“å…¥åºåˆ—
        input_sequence = np.random.randn(3, 24)  # 3ä¸ªé€šé“ï¼Œ24ä¸ªæ—¶é—´ç‚¹
        
        prediction, _ = predict_with_model(
            "models/multi_channel_lstm_20240101_120000.pth",  # æ›¿æ¢ä¸ºå®é™…çš„æ¨¡å‹è·¯å¾„
            input_sequence
        )
        
        print(f"é¢„æµ‹ç»“æœå½¢çŠ¶: {prediction.shape}")
        
    except Exception as e:
        print(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}") 
import pandas as pd
from django.db import connection
from .models import MonitorTask
from .multi_channel_lstm import MultiChannelSensorPreprocessor, MultiChannelLSTM, MultiChannelTrainer, create_data_loaders
import torch
import os
import json
from datetime import datetime

class SensorDataLoader:
    """ä¼ æ„Ÿå™¨æ•°æ®åŠ è½½å™¨"""
    
    def __init__(self):
        # æ”¯æŒCSVæ–‡ä»¶ä¸­çš„é€šé“åˆ—åï¼ˆCH0, CH1, CH2, ...ï¼‰
        self.channel_mapping = {}
        for i in range(16):  # æ”¯æŒ16ä¸ªé€šé“
            self.channel_mapping[f'CH{i}'] = f'CH{i}'
        
        # ä¹Ÿæ”¯æŒä¼ ç»Ÿçš„ä¼ æ„Ÿå™¨åç§°ï¼ˆå‘åå…¼å®¹ï¼‰
        traditional_channels = {
            'temperature': 'temperature',
            'humidity': 'humidity', 
            'acceleration': 'acceleration',
            'voltage': 'voltage',
            'current': 'current',
            'power': 'power',
            'frequency': 'frequency',
            'pressure': 'pressure',
            'flow_rate': 'flow_rate',
            'level': 'level'
        }
        self.channel_mapping.update(traditional_channels)
    
    def load_sensor_data(self, task_ids, channels, start_time=None, end_time=None):
        """
        ä»CSVæ–‡ä»¶åŠ è½½å¤šé€šé“ä¼ æ„Ÿå™¨æ•°æ®
        
        Args:
            task_ids: ä»»åŠ¡IDåˆ—è¡¨
            channels: é€šé“åç§°åˆ—è¡¨ï¼Œå¦‚ ['temperature', 'humidity', 'acceleration']
            start_time: å¼€å§‹æ—¶é—´ (datetimeå¯¹è±¡)
            end_time: ç»“æŸæ—¶é—´ (datetimeå¯¹è±¡)
        
        Returns:
            data: numpyæ•°ç»„ï¼Œå½¢çŠ¶ä¸º (channels, timesteps)
            channel_names: é€šé“åç§°åˆ—è¡¨
        """
        print(f"å¼€å§‹åŠ è½½ä¼ æ„Ÿå™¨æ•°æ®...")
        print(f"ä»»åŠ¡ID: {task_ids}")
        print(f"é€šé“: {channels}")
        
        # éªŒè¯é€šé“
        valid_channels = []
        for channel in channels:
            if channel in self.channel_mapping:
                valid_channels.append(channel)
            else:
                print(f"è­¦å‘Š: æœªçŸ¥é€šé“ '{channel}'ï¼Œå·²è·³è¿‡")
        
        if not valid_channels:
            raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„é€šé“")
        
        # ä»ä»»åŠ¡ä¸­è·å–CSVæ–‡ä»¶è·¯å¾„
        all_data = []
        channel_names = []
        
        for task_id in task_ids:
            try:
                task = MonitorTask.objects.get(task_id=task_id)
                csv_file_path = task.csv_file_path
                
                if not os.path.exists(csv_file_path):
                    print(f"è­¦å‘Š: CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_file_path}")
                    continue
                
                # è¯»å–CSVæ–‡ä»¶
                df = pd.read_csv(csv_file_path)
                print(f"ä» {csv_file_path} åŠ è½½æ•°æ®ï¼Œå½¢çŠ¶: {df.shape}")
                
                # æå–æŒ‡å®šé€šé“çš„æ•°æ®
                for channel in valid_channels:
                    if channel in df.columns:
                        channel_data = df[channel].values
                        all_data.append(channel_data)
                        channel_names.append(f"{task_id}_{channel}")
                    else:
                        print(f"è­¦å‘Š: CSVæ–‡ä»¶ä¸­æ²¡æœ‰ '{channel}' åˆ—")
                
            except MonitorTask.DoesNotExist:
                print(f"è­¦å‘Š: ä»»åŠ¡ID '{task_id}' ä¸å­˜åœ¨")
            except Exception as e:
                print(f"è­¦å‘Š: åŠ è½½ä»»åŠ¡ '{task_id}' æ•°æ®æ—¶å‡ºé”™: {e}")
        
        if not all_data:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ•°æ®")
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        data = np.array(all_data)
        
        print(f"æ•°æ®åŠ è½½å®Œæˆ: {data.shape[0]}ä¸ªé€šé“, {data.shape[1]}ä¸ªæ—¶é—´ç‚¹")
        print(f"é€šé“åç§°: {channel_names}")
        
        return data, channel_names
    
    def load_data_by_task(self, task_id, channels=None):
        """
        æ ¹æ®ä»»åŠ¡IDåŠ è½½æ•°æ®
        
        Args:
            task_id: ä»»åŠ¡ID
            channels: é€šé“åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™åŠ è½½æ‰€æœ‰å¯ç”¨é€šé“
        
        Returns:
            data: numpyæ•°ç»„
            channel_names: é€šé“åç§°åˆ—è¡¨
        """
        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å­˜åœ¨
        try:
            task = MonitorTask.objects.get(task_id=task_id)
            print(f"ä»»åŠ¡ä¿¡æ¯: {task.task_name}")
        except MonitorTask.DoesNotExist:
            raise ValueError(f"ä»»åŠ¡ID '{task_id}' ä¸å­˜åœ¨")
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šé€šé“ï¼Œåˆ™è·å–æ‰€æœ‰å¯ç”¨é€šé“
        if channels is None:
            channels = self._get_available_channels(task_id)
        
        return self.load_sensor_data([task_id], channels)
    
    def _get_available_channels(self, task_id):
        """è·å–ä»»åŠ¡ä¸­å¯ç”¨çš„é€šé“"""
        try:
            task = MonitorTask.objects.get(task_id=task_id)
            csv_file_path = task.csv_file_path
            
            if not os.path.exists(csv_file_path):
                print(f"è­¦å‘Š: CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_file_path}")
                return []
            
            # è¯»å–CSVæ–‡ä»¶å¤´éƒ¨æ¥è·å–å¯ç”¨åˆ—
            df = pd.read_csv(csv_file_path, nrows=1)  # åªè¯»å–ç¬¬ä¸€è¡Œæ¥è·å–åˆ—å
            
            available_channels = []
            # æŸ¥æ‰¾æ‰€æœ‰CHå¼€å¤´çš„åˆ—ï¼ˆå¦‚CH0, CH1, CH2, ...ï¼‰
            for column in df.columns:
                if column.startswith('CH') and column[2:].isdigit():
                    available_channels.append(column)
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°CHæ ¼å¼çš„åˆ—ï¼Œå°è¯•æŸ¥æ‰¾ä¼ ç»Ÿçš„ä¼ æ„Ÿå™¨åç§°
            if not available_channels:
                for channel in self.channel_mapping.keys():
                    if channel in df.columns:
                        available_channels.append(channel)
            
            print(f"å¯ç”¨é€šé“: {available_channels}")
            return available_channels
            
        except MonitorTask.DoesNotExist:
            print(f"è­¦å‘Š: ä»»åŠ¡ID '{task_id}' ä¸å­˜åœ¨")
            return []
        except Exception as e:
            print(f"è­¦å‘Š: è·å–å¯ç”¨é€šé“æ—¶å‡ºé”™: {e}")
            return []

def train_multi_channel_model(task_ids, channels, model_config=None, save_dir="models", progress_callback=None, training_set_id=None):
    """
    è®­ç»ƒå¤šé€šé“LSTMæ¨¡å‹
    
    Args:
        task_ids: ä»»åŠ¡IDåˆ—è¡¨
        channels: é€šé“åç§°åˆ—è¡¨
        model_config: æ¨¡å‹é…ç½®å­—å…¸
        save_dir: æ¨¡å‹ä¿å­˜ç›®å½•
        progress_callback: è®­ç»ƒè¿›åº¦å›è°ƒå‡½æ•°
        training_set_id: è®­ç»ƒé›†IDï¼Œç”¨äºæ£€æŸ¥åœæ­¢çŠ¶æ€
    
    Returns:
        trainer: è®­ç»ƒå¥½çš„è®­ç»ƒå™¨å¯¹è±¡
        metadata: æ¨¡å‹å…ƒæ•°æ®
    """
    print(f"ğŸš€ å¼€å§‹å¤šé€šé“LSTMæ¨¡å‹è®­ç»ƒ...")
    print(f"ğŸ“Š ä»»åŠ¡ID: {task_ids}")
    print(f"ğŸ“Š é€šé“: {channels}")
    print(f"ğŸ“Š æ¨¡å‹é…ç½®: {model_config}")
    print(f"ğŸ“ ä¿å­˜ç›®å½•: {save_dir}")
    if training_set_id:
        print(f"ğŸ†” è®­ç»ƒé›†ID: {training_set_id}")
    
    try:
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        print(f"ğŸ“¦ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
        loader = SensorDataLoader()
        
        # åŠ è½½æ•°æ®
        print(f"ğŸ“¥ å¼€å§‹åŠ è½½ä¼ æ„Ÿå™¨æ•°æ®...")
        data, channel_names = loader.load_sensor_data(task_ids, channels)
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼Œå½¢çŠ¶: {data.shape}")
        print(f"âœ… é€šé“åç§°: {channel_names}")
        
        # æ£€æŸ¥æ•°æ®è´¨é‡
        print(f"ğŸ” æ£€æŸ¥æ•°æ®è´¨é‡...")
        invalid_channels = []
        for i, channel in enumerate(channel_names):
            channel_data = data[i, :]
            min_val = channel_data.min()
            max_val = channel_data.max()
            mean_val = channel_data.mean()
            std_val = channel_data.std()
            
            print(f"  {channel}: å½¢çŠ¶={channel_data.shape}, èŒƒå›´=[{min_val:.3f}, {max_val:.3f}], å‡å€¼={mean_val:.3f}, æ ‡å‡†å·®={std_val:.3f}")
            
            # æ£€æŸ¥æ•°æ®æ˜¯å¦æœ‰æ•ˆ
            if max_val == min_val:
                print(f"    âš ï¸  è­¦å‘Š: é€šé“æ•°æ®æ— æ•ˆï¼ˆæ‰€æœ‰å€¼ç›¸åŒï¼‰")
                invalid_channels.append(i)
        
        # å¦‚æœæ‰€æœ‰é€šé“éƒ½æ— æ•ˆï¼Œæå‰åœæ­¢
        if len(invalid_channels) == len(channel_names):
            raise ValueError("æ‰€æœ‰é€šé“æ•°æ®éƒ½æ— æ•ˆï¼ˆæ‰€æœ‰å€¼ç›¸åŒï¼‰ï¼Œæ— æ³•è¿›è¡Œè®­ç»ƒã€‚è¯·æ£€æŸ¥ä¼ æ„Ÿå™¨è¿æ¥æˆ–ä½¿ç”¨æœ‰æ•ˆçš„æ•°æ®ã€‚")
        
        # åˆ›å»ºé¢„å¤„ç†å™¨
        print(f"ğŸ”§ åˆ›å»ºæ•°æ®é¢„å¤„ç†å™¨...")
        window_size = model_config.get('window_size', 24)
        horizon = model_config.get('horizon', 12)
        preprocessor = MultiChannelSensorPreprocessor(window_size=window_size, horizon=horizon)
        
        # é¢„å¤„ç†æ•°æ®
        print(f"ğŸ”„ å¼€å§‹æ•°æ®é¢„å¤„ç†...")
        processed_data = preprocessor.fit_transform(data)
        print(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")
        print(f"ğŸ“Š è®­ç»ƒæ•°æ®å½¢çŠ¶: {processed_data['X_train'].shape}")
        print(f"ğŸ“Š éªŒè¯æ•°æ®å½¢çŠ¶: {processed_data['X_val'].shape}")
        print(f"ğŸ“Š æµ‹è¯•æ•°æ®å½¢çŠ¶: {processed_data['X_test'].shape}")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        print(f"ğŸ“¦ åˆ›å»ºPyTorchæ•°æ®åŠ è½½å™¨...")
        batch_size = model_config.get('batch_size', 32)
        
        print(f"ğŸ“Š æ‰¹æ¬¡å¤§å°: {batch_size}")
        
        train_loader, val_loader = create_data_loaders(
            processed_data['X_train'], 
            processed_data['y_train'],
            processed_data['X_val'], 
            processed_data['y_val'],
            batch_size=batch_size
        )
        
        print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ")
        print(f"  - è®­ç»ƒé›†æ‰¹æ¬¡æ•°: {len(train_loader)}")
        print(f"  - éªŒè¯é›†æ‰¹æ¬¡æ•°: {len(val_loader)}")
        
        # åˆ›å»ºæ¨¡å‹
        print(f"ğŸ—ï¸ åˆ›å»ºLSTMæ¨¡å‹...")
        input_size = processed_data['num_channels']  # é€šé“æ•°
        hidden_size = model_config.get('hidden_size', 64)
        num_layers = model_config.get('num_layers', 2)
        dropout = model_config.get('dropout', 0.1)
        horizon = model_config.get('horizon', 12)
        
        print(f"ğŸ“Š è¾“å…¥ç»´åº¦: {input_size}")
        print(f"ğŸ“Š éšè—å±‚å¤§å°: {hidden_size}")
        print(f"ğŸ“Š LSTMå±‚æ•°: {num_layers}")
        print(f"ğŸ“Š Dropoutç‡: {dropout}")
        print(f"ğŸ“Š é¢„æµ‹æ­¥é•¿: {horizon}")
        
        # åˆ›å»ºLSTMæ¨¡å‹
        model = MultiChannelLSTM(
            num_channels=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            horizon=horizon,
            dropout=dropout
        )
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = MultiChannelTrainer(model)
        
        # å¼€å§‹è®­ç»ƒ
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒ...")
        history = trainer.train(
            train_loader, 
            val_loader, 
            epochs=model_config.get('epochs', 100),
            lr=model_config.get('lr', 0.001),
            patience=model_config.get('patience', 10),
            progress_callback=progress_callback,
            training_set_id=training_set_id
        )
        
        # æ£€æŸ¥è®­ç»ƒæ˜¯å¦è¢«åœæ­¢
        if history.get('status') == 'stopped':
            print(f"â¹ï¸ è®­ç»ƒå·²è¢«åœæ­¢")
            return trainer, {
                'model_path': None,
                'channels': channel_names,
                'data_info': {
                    'window_size': window_size,
                    'horizon': horizon,
                    'input_size': input_size,
                    'data_shape': data.shape
                },
                'model_config': model_config,
                'training_history': history,
                'channel_stats': processed_data['channel_stats'],
                'test_data': {
                    'X_test': processed_data['X_test'],
                    'y_test': processed_data['y_test']
                },
                'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S'),
                'status': 'stopped'
            }
        
        print(f"âœ… è®­ç»ƒå®Œæˆï¼")
        print(f"â° è®­ç»ƒç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # ä¿å­˜æ¨¡å‹
        print(f"ğŸ’¾ ä¿å­˜æ¨¡å‹...")
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        model_filename = f"multi_channel_lstm_{timestamp}.pth"
        model_path = os.path.join(save_dir, model_filename)
        
        metadata = {
            'model_path': model_path,
            'channels': channel_names,
            'data_info': {
                'window_size': window_size,
                'horizon': horizon,
                'input_size': input_size,
                'data_shape': data.shape
            },
            'model_config': model_config,
            'training_history': history,
            'channel_stats': processed_data['channel_stats'],
            'test_data': {
                'X_test': processed_data['X_test'].tolist(),  # è½¬æ¢ä¸ºåˆ—è¡¨
                'y_test': processed_data['y_test'].tolist()   # è½¬æ¢ä¸ºåˆ—è¡¨
            },
            'timestamp': timestamp,
        }
        
        trainer.save_model(model_path, metadata)
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
        
        # æ‰“å°è®­ç»ƒç»“æœ
        print(f"ğŸ“Š è®­ç»ƒç»“æœæ€»ç»“:")
        if history and 'train_loss' in history and len(history['train_loss']) > 0:
            print(f"  - æœ€ç»ˆè®­ç»ƒæŸå¤±: {history['train_loss'][-1]:.6f}")
        if history and 'val_loss' in history and len(history['val_loss']) > 0:
            print(f"  - æœ€ç»ˆéªŒè¯æŸå¤±: {history['val_loss'][-1]:.6f}")
        if history and 'best_val_loss' in history:
            print(f"  - æœ€ä½³éªŒè¯æŸå¤±: {history['best_val_loss']:.6f}")
        if history and 'epochs_trained' in history:
            print(f"  - è®­ç»ƒè½®æ•°: {history['epochs_trained']}")
        
        return trainer, metadata
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        raise e

def load_trained_model(model_path):
    """
    åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    
    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
    
    Returns:
        trainer: è®­ç»ƒå™¨å¯¹è±¡
        metadata: æ¨¡å‹å…ƒæ•°æ®
    """
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
    
    # é¦–å…ˆåŠ è½½æ¨¡å‹æ•°æ®æ¥è·å–é…ç½®
    save_data = torch.load(model_path, map_location='cpu', weights_only=False)
    model_config = save_data.get('model_config', {})
    
    # ä»æ¨¡å‹é…ç½®ä¸­è·å–é€šé“æ•°ï¼Œå¿…é¡»å­˜åœ¨
    if 'num_channels' not in model_config:
        raise ValueError(f"æ¨¡å‹é…ç½®ä¸­ç¼ºå°‘num_channelså­—æ®µ: {model_config}")
    
    num_channels = model_config['num_channels']
    hidden_size = model_config.get('hidden_size', 64)
    num_layers = model_config.get('num_layers', 2)
    horizon = model_config.get('horizon', 12)
    
    print(f"ğŸ“Š ä»æ¨¡å‹æ–‡ä»¶ä¸­è¯»å–çš„é…ç½®:")
    print(f"  - é€šé“æ•°: {num_channels}")
    print(f"  - éšè—å±‚å¤§å°: {hidden_size}")
    print(f"  - LSTMå±‚æ•°: {num_layers}")
    print(f"  - é¢„æµ‹æ­¥é•¿: {horizon}")
    
    # ä½¿ç”¨æ­£ç¡®çš„é…ç½®åˆ›å»ºdummyæ¨¡å‹
    dummy_model = MultiChannelLSTM(
        num_channels=num_channels,
        hidden_size=hidden_size,
        num_layers=num_layers,
        horizon=horizon
    )
    trainer = MultiChannelTrainer(dummy_model)
    
    # åŠ è½½æ¨¡å‹
    metadata = trainer.load_model(model_path)
    
    # ç¡®ä¿metadataåŒ…å«æ­£ç¡®çš„model_configï¼ˆä½¿ç”¨æ¨¡å‹æ–‡ä»¶ä¸­çš„é…ç½®ï¼‰
    metadata['model_config'] = model_config
    
    return trainer, metadata

def predict_with_model(model_path, input_sequence, denormalize=True):
    """
    ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
    
    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        input_sequence: è¾“å…¥åºåˆ—ï¼Œå½¢çŠ¶ä¸º (channels, window_size)
        denormalize: æ˜¯å¦åå½’ä¸€åŒ–é¢„æµ‹ç»“æœ
    
    Returns:
        prediction: é¢„æµ‹ç»“æœ
        metadata: æ¨¡å‹å…ƒæ•°æ®
    """
    # åŠ è½½æ¨¡å‹
    trainer, metadata = load_trained_model(model_path)
    model = trainer.model
    
    # ç¡®ä¿è¾“å…¥æ ¼å¼æ­£ç¡®
    if isinstance(input_sequence, np.ndarray):
        input_sequence = torch.FloatTensor(input_sequence)
    
    # æ·»åŠ batchç»´åº¦
    if input_sequence.dim() == 2:  # (channels, window_size)
        input_sequence = input_sequence.unsqueeze(0)  # (1, channels, window_size)
    
    # é¢„æµ‹
    model.eval()
    with torch.no_grad():
        prediction = model(input_sequence)
        prediction = prediction.squeeze(0).cpu().numpy()  # (channels, horizon)
    
    # åå½’ä¸€åŒ–
    if denormalize and 'channel_stats' in metadata:
        prediction = denormalize_prediction(prediction, metadata['channel_stats'])
    
    return prediction, metadata

def denormalize_prediction(prediction, channel_stats):
    """åå½’ä¸€åŒ–é¢„æµ‹ç»“æœ"""
    denormalized = prediction.copy()
    
    for channel in range(prediction.shape[0]):
        if str(channel) in channel_stats:
            stats = channel_stats[str(channel)]
            min_val = stats['min']
            max_val = stats['max']
            
            # åå½’ä¸€åŒ–: normalized * (max - min) + min
            denormalized[channel, :] = prediction[channel, :] * (max_val - min_val) + min_val
    
    return denormalized

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # ç¤ºä¾‹ï¼šè®­ç»ƒæ¨¡å‹
    task_ids = ['task1', 'task2']  # æ›¿æ¢ä¸ºå®é™…çš„ä»»åŠ¡ID
    channels = ['temperature', 'humidity', 'acceleration']  # æ›¿æ¢ä¸ºå®é™…çš„é€šé“
    
    try:
        trainer, metadata = train_multi_channel_model(task_ids, channels)
        print("è®­ç»ƒæˆåŠŸå®Œæˆï¼")
        
        # ç¤ºä¾‹ï¼šä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹
        # å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªè¾“å…¥åºåˆ—
        input_sequence = np.random.randn(3, 24)  # 3ä¸ªé€šé“ï¼Œ24ä¸ªæ—¶é—´ç‚¹
        
        prediction, _ = predict_with_model(
            "models/multi_channel_lstm_20240101_120000.pth",  # æ›¿æ¢ä¸ºå®é™…çš„æ¨¡å‹è·¯å¾„
            input_sequence
        )
        
        print(f"é¢„æµ‹ç»“æœå½¢çŠ¶: {prediction.shape}")
        
    except Exception as e:
        print(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}") 
 
 